## llama.cpp/cuda-default

Container: https://hub.docker.com/r/ggml/llama-cpp-cuda-default

```bash
docker build --platform linux/amd64 -t ggml/llama-cpp-cuda-default . --no-cache
docker push ggml/llama-cpp-cuda-default
```
